from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader
import os
import json
import httpx
import asyncio
from typing import List, Dict, Any
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

class MCPClient:
    """Client l√©ger pour communiquer avec des serveurs MCP via SSE/HTTP"""
    def __init__(self, endpoint: str):
        self.endpoint = endpoint.rstrip('/')
        
    async def get_tools(self) -> List[Dict[str, Any]]:
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.endpoint}/tools")
                if response.status_code == 200:
                    return response.json().get("tools", [])
        except Exception as e:
            print(f"Erreur MCP List Tools: {str(e)}")
        return []

    async def call_tool(self, tool_name: str, arguments: Dict[str, Any]) -> Any:
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.endpoint}/call", 
                    json={"name": tool_name, "arguments": arguments}
                )
                if response.status_code == 200:
                    return response.json().get("result")
                return f"Erreur MCP ({response.status_code}): {response.text}"
        except Exception as e:
            print(f"Erreur MCP Call Tool: {str(e)}")
            return f"Erreur lors de l'appel √† l'outil {tool_name}: {str(e)}"

load_dotenv(dotenv_path="../.env.local")

class WhatsAppChatbot:
    def __init__(self):
        # Initialisation du LLM
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.embeddings = OpenAIEmbeddings()
        self.vector_store = None
        self.retriever = None
        
        # Charger la base de connaissances
        self._initialize_knowledge()

    def _initialize_knowledge(self):
        knowledge_path = "knowledge.txt"
        if not os.path.exists(knowledge_path):
            print("Erreur: knowledge.txt introuvable")
            return

        with open(knowledge_path, "r", encoding="utf-8") as f:
            content = f.read()

        # Simple d√©coupage par paragraphe pour cet exemple
        texts = [p.strip() for p in content.split("\n\n") if p.strip()]
        
        # Cr√©ation du magasin de vecteurs local (FAISS)
        self.vector_store = FAISS.from_texts(texts, self.embeddings)
        self.retriever = self.vector_store.as_retriever(search_kwargs={"k": 2})
        print("‚úÖ Base de connaissances charg√©e et index√©e.")

    async def ask_with_sources(self, question: str, instructions: str, urls: list = None, files: list = None, mcp_endpoints: list = None):
        # 1. Charger le contexte des URLs et Fichiers
        context_data = ""
        
        # Gestion des URLs
        if urls:
            try:
                loader = WebBaseLoader(web_paths=urls)
                docs = loader.load()
                context_data += "\n\n--- DONN√âES WEB ---\n" + "\n\n".join([doc.page_content for doc in docs])
            except Exception as e:
                print(f"Erreur chargement URL: {str(e)}")

        # Gestion des Fichiers (PDF)
        if files:
            for file_path in files:
                if not file_path or not os.path.exists(file_path): continue
                try:
                    loader = PyPDFLoader(file_path)
                    pages = loader.load_and_split()
                    context_data += f"\n\n--- DOCUMENT: {os.path.basename(file_path)} ---\n"
                    context_data += "\n\n".join([page.page_content for page in pages])
                except Exception as e:
                    print(f"Erreur chargement PDF ({file_path}): {str(e)}")

        # 2. Gestion des outils MCP & Conversion en outils d'agent
        tools_map = {}
        if mcp_endpoints:
            for endpoint in mcp_endpoints:
                client = MCPClient(endpoint)
                mcp_tools = await client.get_tools()
                for t in mcp_tools:
                    name = t.get('name')
                    tools_map[name] = {
                        "client": client,
                        "description": t.get('description'),
                        "parameters": t.get('inputSchema', t.get('parameters', {}))
                    }
                    available_tools_info += f"- Outil: {name} | Description: {t.get('description')}\n"

        # 3. Pr√©parer le prompt robuste
        prompt = ChatPromptTemplate.from_messages([
            ("system", f"{instructions}\n\nSOURCES :\n{context_data[:8000]}\n\nOUTILS DISPONIBLES :\n{available_tools_info if available_tools_info else 'Aucun'}\n\nIMPORTANT: Si un outil est n√©cessaire pour r√©pondre (ex: v√©rifier un calendrier, envoyer un email), utilise-le avant de donner la r√©ponse finale."),
            ("human", "{question}")
        ])
        
        # 4. Ex√©cution avec gestion des outils (Loop simple)
        # Note: Pour une version plus complexe, utiliser LangGraph ou create_tool_calling_agent
        current_llm = self.llm
        if tools_map:
            # On informe le LLM des outils au format JSON pour qu'il puisse sugg√©rer l'appel
            # Dans cette version simplifi√©e, on demande au LLM d'indiquer l'outil √† appeler dans un format sp√©cifique
            prompt = ChatPromptTemplate.from_template(f"""{instructions}
            
CONNAISSANCES (RAG):
{context_data[:10000]}

OUTILS MCP DISPONIBLES:
{available_tools_info}

REÃÄGLES:
1. Si tu as besoin d'un outil pour r√©pondre, √©cris UNIQUEMENT: TOOL_CALL: {{"name": "nom_outil", "arguments": {{"arg": "val"}}}}
2. Si tu as toutes les infos pour reÃÅpondre, donne la reÃÅponse finale au client.
3. Toujours r√©pondre en fran√ßais.

QUESTION DU CLIENT: {{question}}
REÃÅPONSE:""")

        chain = prompt | self.llm | StrOutputParser()
        response = await chain.ainvoke({"question": question})

        # 5. Gestion de l'appel d'outil (Boucle 1 it√©ration pour l'instant)
        if "TOOL_CALL:" in response:
            try:
                call_json = response.split("TOOL_CALL:")[1].strip()
                call_data = json.loads(call_json)
                tool_name = call_data.get("name")
                args = call_data.get("arguments", {})
                
                if tool_name in tools_map:
                    print(f"üõ†Ô∏è Appel de l'outil MCP: {tool_name}")
                    tool_result = await tools_map[tool_name]["client"].call_tool(tool_name, args)
                    
                    # Refaire un appel au LLM avec le r√©sultat de l'outil
                    prompt_with_result = ChatPromptTemplate.from_template(f"""{instructions}
                    
R√âSULTAT DE L'OUTIL ({tool_name}):
{json.dumps(tool_result, indent=2, ensure_ascii=False)}

QUESTION INITIALE: {{question}}

En tenant compte du r√©sultat ci-dessus, r√©ponds au client de mani√®re chaleureuse et professionnelle sur WhatsApp (utilise des emojis).""")
                    
                    final_chain = prompt_with_result | self.llm | StrOutputParser()
                    return await final_chain.ainvoke({"question": question})
            except Exception as e:
                print(f"Erreur tool execution: {str(e)}")
                return f"D√©sol√©, j'ai rencontr√© une erreur technique en utilisant mes outils: {response}"

        return response

    async def ask_with_custom_knowledge(self, question: str, custom_content: str):
        if not custom_content:
            return self.ask(question)

        # Si le contenu est court (< 2000 caract√®res), on l'injecte directement dans le prompt
        # Sinon, on cr√©e un index FAISS temporaire pour cette requ√™te
        if len(custom_content) < 2000:
            prompt = ChatPromptTemplate.from_template("""
Tu es un assistant WhatsApp. R√©ponds en utilisant UNIQUEMENT les infos ci-dessous.
INFO ENTREPRISE:
{context}

QUESTION: {question}
R√âPONSE:""")
            chain = prompt | self.llm | StrOutputParser()
            return await chain.ainvoke({"context": custom_content, "question": question})
        else:
            # RAG Dynamique pour gros volumes de texte
            texts = [p.strip() for p in custom_content.split("\n\n") if p.strip()]
            temp_db = FAISS.from_texts(texts, self.embeddings)
            retriever = temp_db.as_retriever(search_kwargs={"k": 3})
            
            prompt = ChatPromptTemplate.from_template("""
Utilise les extraits suivants pour r√©pondre √† la question.
CONTEXTE:
{context}

QUESTION: {question}
R√âPONSE:""")
            chain = ({"context": retriever, "question": RunnablePassthrough()} | prompt | self.llm | StrOutputParser())
            return await chain.ainvoke(question)

    def ask(self, question: str):
        if not self.retriever:
            return "D√©sol√©, ma base de connaissances n'est pas pr√™te."

        template = """Tu es un assistant WhatsApp pour Wozif Connect. 
R√©ponds UNIQUEMENT en utilisant les informations fournies ci-dessous.
Si tu ne connais pas la r√©ponse, dis-le poliment et propose au client de contacter le support √† support@wozif.com.
Sois concis et utilise des emojis pour rendre la r√©ponse agr√©able sur WhatsApp.

CONTEXTE:
{context}

QUESTION DU CLIENT:
{question}

R√âPONSE:"""
        
        prompt = ChatPromptTemplate.from_template(template)
        
        # Cha√Æne RAG
        chain = (
            {"context": self.retriever, "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        return chain.invoke(question)

# --- TEST LOCAL ---
if __name__ == "__main__":
    bot = WhatsAppChatbot()
    
    questions = [
        "C'est quoi Wozif Connect ?",
        "Quels sont vos tarifs ?",
        "Comment vous contacter ?",
        "Est-ce que vous vendez des pizzas ?" 
    ]
    
    for q in questions:
        print(f"\n‚ùì Client: {q}")
        response = bot.ask(q)
        print(f"ü§ñ Bot: {response}")
